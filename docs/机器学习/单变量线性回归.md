# 单变量线性回归

> 因为只含有一个特征/输入变量,因此这样的问题叫作单变量线性回归。

## 模型描述
- 经常使用的一些符号定义：
   - m 代表训练集中实例的数量
   - x 代表特征/输入变量
   - y 代表目标变量/输出变量
   - (x,y) 代表训练集中的实例
   - (x^(i),y^(i)) 代表第i个观察实例
   - h 代表学习算法的解决方案或函数也称为假设(hypothesis)

- 监督学习的工作过程：
   - 我们向学习算法提供训练集，学习算法的任务是输出一个函数h（假设函数），作用为把x作为输入变量输出相应的预测y值（引导从x得到y）。

## 代价函数
- 假设函数： hθ(x) = θ0 +θ1x
   - θ0 和 θ1 通常称其为模型参数。
   - 选择不同的参数θ0和θ1我们会得到不同的假设函数。

- 在线性回归中，我们要解决的是一个最小化问题
   - 选择的参数 θ0 和 θ1 决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距就是建模误差（modeling error）- 例如房子预测卖出价格与实际卖出价格的差的平方 - 为使数字更加直白，一般对这个求和值的二分之一求最小值。
   - 我们的目标便是选择出可以使得建模误差的平方和能够最小的模型参数。即使得代价函数（平方误差函数）：<br>![](https://images0.cnblogs.com/blog/663864/201410/120416270935721.png)
   - 误差平方代价函数对于大多数问题，特别是回归问题，都是一个合理的选择。
   - 若是绘制一个等高线图，三个坐标分别为 θ0 和 θ1 和J(θ0 ,θ1)可以看出在三维空间中存在一个使得J(θ0,θ1)最小的点：<br>
   ![](https://images0.cnblogs.com/blog/663864/201410/120416527809869.png)
   - 等高线图的例子：<br>
   ![](https://images0.cnblogs.com/blog/663864/201410/120417333746870.png)

## 梯度下降
- **梯度下降算法**，该算法可以将代价函数 J 最小化。它不仅被用在线性回归上，并且被广泛的应用于机器学习的众多领域中，之后为了解决其他线性回归问题，我们也将使用梯度下降法来最小化其他函数。
   - 首先我们要做的是对 θ0  和 θ1 进行初始化(实际上，它们到底是什么其实并不重要)。通常的选择是将 θ0 设为0
   ，同时也将 θ1 也设为0，即将它们都初始化为0。我们在梯度下降算法中要做的就是不停地一点点地改变 θ0 和 θ1 ，试图通过这种改变使得 J(θ0 ,θ1) 变小，直到我们找到 J(θ0 ,θ1) 的最小值（或许是局部最小值）。
   - 注意坐标轴 θ0 和 θ1 在水平轴上，而函数 J(θ0 ,θ1) 在垂直坐标轴上，图形表面高度即是 J(θ0 ,θ1) 的值。我们希望最小化这个函数，因此我们从 θ0 和 θ1 的某个值出发。所以想象一下，对 θ0 和 θ1 赋以某个初值，也就是对应于从这个函数表面上的某个起始点出发。
   ![](https://images0.cnblogs.com/blog/663864/201410/120418255623880.png)
   ![](https://images0.cnblogs.com/blog/663864/201410/120419033744366.png)
   - 此外这种下降有一个有趣的特点，不同的起始点开始进行梯度下降算法，你会得到一个非常不同的局部最优解，这就是梯度下降算法的一个特点。

- **梯度下降算法的定义**：
   ![](https://images0.cnblogs.com/blog/663864/201410/120419518439473.png)
   - 符号：=表示赋值，符号=表示真假判断。
   - α是学习率，用来控制梯度下降时候的步长。
   - 不断**同时**更新θ0与更新θ1，如上图下式，要做到同步更新。

- 梯度下降算法中更新时减去的项（α*偏导数项）的解释
   - 偏导数项可以令下一个更新数更接近最低点（正负）。
   - 如果α学习速率太小，这样就需要很多步才能到达最低点。如果α学习速率太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛。实际上，会离最低点越来越远，因此，如果α太大，会导致无法收敛，甚至发散。
   - 如果你的参数已经处于局部最低点，那么梯度下降法更新其实什么都没做（求导为0），它不会改变参数的值，这也正是你想要的，因为它使你的解始终保持在局部最优点，这也解释了为什么即使学习速率 α 保持不变时，梯度下降也可以收敛到局部最低点。
   - 梯度下降一步后，新的导数会变小一点点。随着梯度下降法的运行，你移动的幅度会自动变得越来越小，直到最终移动幅度非常小，你会发现已经收敛到局部极小值。
   在梯度下降法中，当我们接近局部最低点时，梯度下降法会自动采取更小的幅度。这是因为当我们接近局部最低点时（很显然在局部最低时导数等于零 ），导数值会自动变得越来越小，所以梯度下降将自动采取较小的幅度，这就是梯度下降的做法，所以实际上没有必要再另外减小α。
   ![](https://images0.cnblogs.com/blog/663864/201410/120421081713359.png)

- 线性回归的梯度下降
   - 偏导数项计算：
   ![](https://images0.cnblogs.com/blog/663864/201410/120421379211981.png)
   代入原来的算法即为：
   ![](https://images0.cnblogs.com/blog/663864/201410/120421593902439.png)
   - 实际上，用于线性回归的代价函数总是呈现一个弓形的样子，这个函数的专业术语是凸函数。这个函数没有任何局部最优解，只有一个全局最优解。
   ![](https://images0.cnblogs.com/blog/663864/201410/120422292808805.png)
   - Batch梯度下降意味着每一次梯度下降都遍历了整个训练集的样本。

