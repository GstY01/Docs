### 逻辑回归 Logistic Regression

+ 分类算法，二分类问题和多分类问题

+ 二分类决策
  $$
  z=(\sum_{i=1}^nw_ix_i)+b
  $$

  $$
  W = [w_1,w_2,\dots,w_n],X=[x_1,x_2,\dots,x_n]
  $$

  $$
  z=W\cdot X+b
  $$

  $$
  \sigma(z)=\frac{1}{1+e^{-z}}
  $$

+ sigmoid

  ![sigmoid](D:\edu\ml\sigmoid.jpg)
  $$
  P(y=1) = \sigma(z) =\sigma(W\cdot X +b) = \frac{1}{1+e^{-(W\cdot X+b)}}
  $$

  $$
  P(y=0) = 1-P(y=1) = 1-\sigma(W\cdot X+b) =\frac{e^{-(W\cdot X+b)}}{1+e^{-(W\cdot X+b)}}
  $$

+ 损失函数
  $$
  L(\hat{y},y)=-y\log\hat{y}-(1-y)\log(1-\hat{y})
  $$

  $$
  \hat{y}=\sigma(W\cdot X+b)
  $$

  $$
  cost(W,b) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)}) = -\frac{1}{m}\sum_{i=1}^m y^{(i)}\log\sigma(W\cdot X^{(i)}) + (1-\hat{y}^{(i)})\log(1-\sigma(W\cdot X^{(i)}+b))
  $$

+ 凸函数，梯度下降
  $$
  L(W,b)=-[y\log\sigma(W\cdot X+b)+(1-y)\log(1-\sigma(W\cdot X+b))]
  $$

  $$
  \frac{\partial L(W,b)}{\partial w_j} =\frac{\partial L(W,b)}{\partial \sigma}\frac{\partial \sigma}{\partial w_j}= [\sigma(W\cdot X+b)-y]x_j
  $$

  $$
  \frac{\partial cost(W,b)}{\partial w_j} = \sum_{i=1}^m[\sigma(W\cdot X^{(i)}+b)-y^{(i)}]X_j^{(i)}
  $$

+ 多分类决策

  + 多个二分器策略

    OvO（one vs one）

    ![ovo](D:\edu\ml\ovo.png)

    OvR (One vs Rest)

    ![ovr](D:\edu\ml\ovr.png)

    MvM (Many vs Many)

  + softmax
    $$
    \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^ke^{z_j}}
    $$

    $$
    z_i=W_i\cdot x+b_i
    $$

    $$
    p(y=c) = \frac{e^{W_c\cdot X+b_c}}{\sum_{j=1}^k e^{W_j\cdot X+b_j}}
    $$

### 支持向量机

+ 间隔最大化

<img src="D:\edu\ml\svm.png" style="zoom: 67%;" />

+ 线性可分SVM，硬间隔(hard margin)最大化

  <img src="D:\edu\ml\hard margin.png" alt="hard margin" style="zoom: 25%;" />

+ 线性SVM，软间隔(soft margin)最大化

+ 非线性SVM，核技巧(kernel trick)和软间隔最大化

  ![](D:\edu\ml\kerneltrick.png)

$$
\left[
\begin{matrix}
 1      & 2      & \cdots & 4      \\
 7      & 6      & \cdots & 5      \\
 \vdots & \vdots & \ddots & \vdots \\
 8      & 9      & \cdots & 0      \\
\end{matrix}
\right]
$$

